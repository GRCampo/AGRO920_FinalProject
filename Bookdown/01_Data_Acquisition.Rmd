---
title: "Data Acquisition"
output: bookdown::html_document2
bibliography: packages.bib
---
# Data Acquisition

## Download

The dataset used in this study was generated by Liu et al. (2015) and is publicly available at the NCBI Sequence Read Archive (SRA) under the BioProject accession [PRJNA271170](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA271170). 

To download the 14 paired-end RNA-seq samples, I used `fasterq-dump` from the SRA Toolkit, wrapped in a SLURM job array to parallelize the process (because downloading 190 GB on a single core sounded like a terrible idea).

Each file was compressed after download to save space.

**Total size**: approximately **190 GB** (uncompressed).  
**Output**: 28 gzipped FASTQ files (`*_1.fastq.gz` and `*_2.fastq.gz` for each sample).

> **Note:** The download script is stored in the `scripts` directory as `01_runDownload.sbatch`.

```{bash, eval=FALSE}
#!/bin/bash -l
#SBATCH --job-name=data920_download
#SBATCH --time=1-00:00:00
#SBATCH --mem=50G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=10
#SBATCH --array=1-14

RUNS=("SRR1542404" "SRR1542405" "SRR1542406" "SRR1542407" "SRR1542408" "SRR1542409" "SRR1542410" "SRR1542411" "SRR1542412" "SRR1542413" "SRR1542414" "SRR1542415" "SRR1542416" "SRR1542417")

RUN=${RUNS[$SLURM_ARRAY_TASK_ID-1]}

echo "Processing $RUN Job Array ID: $SLURM_ARRAY_TASK_ID"

fasterq-dump --split-files --threads 10 $RUN

gzip "${RUN}_1.fastq"
gzip "${RUN}_2.fastq"

echo "$RUN done!"
```