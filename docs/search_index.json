[["index.html", "Models 1 About 1.1 Data 1.2 Project Overview", " Models Gabriela Romero Campos 2025-05-14 1 About This project was developed as part of the AGRO 920 – Predictive Modeling for Plant Breeding course at Kansas State University. The goal wasn’t to reinvent the wheel, but to show that I can work with complex biological data and apply machine learning models in a meaningful way. In short: build a working pipeline, survive the process, and (hopefully) get useful results. The project combines plant genomics, transcriptomics, and predictive modeling to find patterns in RNA-seq data related to abiotic stress in wheat (Triticum aestivum). Instead of using a reference genome, I followed a reference free approach: breaking raw reads into k-mers and using them directly as input for machine learning models. It’s a bit unconventional, but sometimes going around the map is faster than sticking to the official roads. All scripts, data processing steps, and model evaluations are clearly documented — partly for reproducibility, and partly so future-me doesn’t hate past-me when trying to understand what I did. The raw data and intermediate files are stored in the AGRO920 folder of my Beocat user directory. If you’d like access, feel free to ask, I’ll gladly give permission. The results and scripts folders are also available on my GitHub for anyone interested in the pipeline or plots without digging through a cluster. 1.1 Data The dataset used in this project comes from Liu et al. (2015), a study that investigated transcriptional responses of wheat under drought (DS), heat (HS), and combined heat and drought (HD) stress. The experiment used the TAM 107 cultivar, sampled at two time points (1h and 6h after stress onset), and focused on leaf tissue, where most of the drama tends to happen under stress. In total, 14 samples were sequenced, each with replicates, producing about 900 million 100-bp paired-end reads via the Illumina platform. A small and quiet dataset (~contains irony). For this project, the raw RNA-seq reads were downloaded from the NCBI SRA and processed through a reference-free, k-mer-based pipeline that is equal parts practical and a little bit original. The steps: Quality checks were performed with FastQC, MultiQC, and MarkDuplicates. Reads were converted to FASTA and split into pseudo-samples (100 files per sample) to simulate replicates and make downstream modeling less of a statistical nightmare. Jellyfish (v2.3.0) was used to count k-mers with a length of 41. The resulting k-mer matrix was filtered by abundance (count &gt; 10), variability (top 25% CV), and redundancy (95% similarity via CD-HIT), leaving behind a lean set of 13,490 representative k-mers. Then, I used this k-mer matrix for: Exploratory analysis with PCA and t-SNE, to see whether the pseudo-samples wanted to behave and cluster by treatment. Supervised modeling using Random Forest, to classify stress conditions and exposure times — or at least try to. Feature selection, to highlight the most informative k-mers. 1.2 Project Overview 1.2.1 Objective This project aims to identify k-mers associated with specific abiotic stress conditions and time points in wheat using a machine learning framework. The central hypothesis? That raw RNA-seq reads contain hidden sequence patterns. Patterns that can be detected at the k-mer level, which reflect how the plant is responding to heat, drought, or the lovely combination of both. To test this, I built a reference-free pipeline that strings together preprocessing, pseudo-sample generation, k-mer extraction, dimensionality reduction, and supervised classification. The goal wasn’t just to classify samples correctly (though that’s nice), but to use the models to highlight the most informative k-mers — those little fragments of sequence that might actually tell us something biologically interesting. 1.2.2 Why k-mers? Instead of relying on annotated genes or transcript quantification, this pipeline starts from scratch, literally from the raw reads. K-mer counting skips the need for a reference genome, which is great news if your species of interest doesn’t have one (or has one that’s… questionable). It also gives you the chance to capture signal from other places as non-coding RNAs, or whatever else the reference might have missed or ignored. It’s messy, but in a good way. 1.2.3 Why pseudo-samples (PS)? The original dataset has only two biological replicates per treatment, which is a great work but, let’s be honest, not ideal for training machine learning models. To work around this, I created pseudosamples (PS): smaller subsets of reads, randomly split from the originals while keeping the paired-end structure intact. Are they true biological replicates? No. Do they help the model see more of the internal variation without cheating? Yes, if you split your training and testing sets carefully (which I did). It’s not perfect, but it’s better than pretending two replicates are enough. 1.2.4 Why machine learning? Because this project lives at the intersection of “too many variables” and “not enough samples,” and that’s exactly the kind of problem machine learning is designed for. Classical tools like edgeR or DESeq2 are great when you want to find single DEGs that behave nicely. But ML models can look at thousands of features at once and pick up more subtle or multivariate signal, the kind you’d never spot with p-values alone. Here, the models are used not just to classify but to learn and prioritize which k-mers matter most, opening the door to future work in stress diagnostics and functional genomics. "],["data-acquisition.html", "2 Data Acquisition 2.1 Download", " 2 Data Acquisition 2.1 Download The dataset used in this study was generated by Liu et al. (2015) and is publicly available at the NCBI Sequence Read Archive (SRA) under the BioProject accession PRJNA271170. To download the 14 paired-end RNA-seq samples, I used fasterq-dump from the SRA Toolkit, wrapped in a SLURM job array to parallelize the process (because downloading 190 GB on a single core sounded like a terrible idea). Each file was compressed after download to save space. Total size: approximately 190 GB (uncompressed). Output: 28 gzipped FASTQ files (*_1.fastq.gz and *_2.fastq.gz for each sample). Note: The download script is stored in the scripts directory as 01_runDownload.sbatch. #!/bin/bash -l #SBATCH --job-name=data920_download #SBATCH --time=1-00:00:00 #SBATCH --mem=50G #SBATCH --nodes=1 #SBATCH --ntasks-per-node=10 #SBATCH --array=1-14 RUNS=(&quot;SRR1542404&quot; &quot;SRR1542405&quot; &quot;SRR1542406&quot; &quot;SRR1542407&quot; &quot;SRR1542408&quot; &quot;SRR1542409&quot; &quot;SRR1542410&quot; &quot;SRR1542411&quot; &quot;SRR1542412&quot; &quot;SRR1542413&quot; &quot;SRR1542414&quot; &quot;SRR1542415&quot; &quot;SRR1542416&quot; &quot;SRR1542417&quot;) RUN=${RUNS[$SLURM_ARRAY_TASK_ID-1]} echo &quot;Processing $RUN Job Array ID: $SLURM_ARRAY_TASK_ID&quot; fasterq-dump --split-files --threads 10 $RUN gzip &quot;${RUN}_1.fastq&quot; gzip &quot;${RUN}_2.fastq&quot; echo &quot;$RUN done!&quot; "],["quality-control.html", "3 Quality Control 3.1 FastQC and MultiQC 3.2 Duplicate Removal with MarkDuplicates", " 3 Quality Control Although this RNA-seq dataset came from a published study, it didn’t include any quality control metrics. So, I ran an independent QC check to make sure the reads was well. This included FastQC, MultiQC, and duplicate removal with MarkDuplicates from the Picard toolkit (all with the goal of reducing technical noise and preserving my sanity for the analyses ahead). 3.1 FastQC and MultiQC Each paired-end FASTQ file was evaluated using FastQC (v0.12.1) via SLURM array jobs. After that, I used MultiQC to bring everything together into one readable summary. #!/bin/bash -l #SBATCH --job-name=data920_fastqc #SBATCH --time=02:00:00 #SBATCH --mem=50G #SBATCH --nodes=1 #SBATCH --ntasks-per-node=10 #SBATCH --array=1-14 # Load Fastqc module load FastQC/0.12.1-Java-11 # finding R1 and R2 files FASTQ_FILES_R1=($(ls /homes/grcampos/AGRO920/data/*_1.fastq.gz)) FASTQ_FILES_R2=($(ls /homes/grcampos/AGRO920/data/*_2.fastq.gz)) # One file per array index FILE_R1=${FASTQ_FILES_R1[$SLURM_ARRAY_TASK_ID - 1]} #bash arrays starts with 0 FILE_R2=${FASTQ_FILES_R2[$SLURM_ARRAY_TASK_ID - 1]} # Running Fatsqc echo &quot;Processing $FILE_R1 and $FILE_R2&quot; fastqc -t 10 -o /homes/grcampos/AGRO920/fastqc_reports &quot;$FILE_R1&quot; &quot;$FILE_R2&quot; module load MultiQC/1.14-foss-2022a sbatch --job-name=mqc920 --time=02:00:00 --mem=10G --nodes=1 --wrap=&quot;multiqc /bulk/eakhunov/grcampos/embeddings/fastqc_reports -o /bulk/eakhunov/grcampos/embeddings/fastqc_reports&quot; Output: All FastQC and MultiQC reports are saved in the fastqc_reports/ directory. Click here to open the MultiQC report 3.2 Duplicate Removal with MarkDuplicates Next, I addressed the possibility of overrepresented sequences, which could skew k-mer frequencies. Using Picard’s MarkDuplicates, I converted the FASTQ files to BAM, marked duplicates, converted them back to deduplicated FASTQ files, and cleaned up all the temporary BAMs to avoid filling up the server (again). #!/bin/bash -l #SBATCH --job-name=data920_markD #SBATCH --time=4-00:00:00 #SBATCH --mem=50G #SBATCH --nodes=1 #SBATCH --ntasks-per-node=10 #SBATCH --array=1-14 module load picard/2.25.1-Java-11 # List of paired-end FASTQ files FASTQ_FILES_R1=($(ls data/*_1.fastq.gz)) FASTQ_FILES_R2=($(ls data/*_2.fastq.gz)) # One file per array index FILE_R1=${FASTQ_FILES_R1[$SLURM_ARRAY_TASK_ID - 1]} FILE_R2=${FASTQ_FILES_R2[$SLURM_ARRAY_TASK_ID - 1]} SAMPLE_NAME=$(basename &quot;$FILE_R1&quot; | sed &#39;s/_1.fastq.gz//&#39;) echo &quot;Processing sample: $SAMPLE_NAME&quot; java -jar $EBROOTPICARD/picard.jar FastqToSam -F1=&quot;$FILE_R1&quot; -F2=&quot;$FILE_R2&quot; O=&quot;bam_files/${SAMPLE_NAME}_unaligned.bam&quot; -SM=&quot;$SAMPLE_NAME&quot; echo &quot;FASTQ to BAM completed for $SAMPLE_NAME&quot; # Remove duplicates with MarkDuplicates java -jar $EBROOTPICARD/picard.jar MarkDuplicates I=&quot;bam_files/${SAMPLE_NAME}_unaligned.bam&quot; O=&quot;bam_files/${SAMPLE_NAME}_deduplicated.bam&quot; M=&quot;bam_files/${SAMPLE_NAME}_duplicate_metrics.txt&quot; REMOVE_DUPLICATES=true VALIDATION_STRINGENCY=LENIENT #Lenient: Display warnings but will continue processing even with minor errors. echo &quot;MarkDuplicates completed for $SAMPLE_NAME&quot; # Convert BAM back to FASTQ java -jar $EBROOTPICARD/picard.jar SamToFastq I=&quot;bam_files/${SAMPLE_NAME}_deduplicated.bam&quot; F=&quot;fastq_filtered/${SAMPLE_NAME}_R1_clean.fastq.gz&quot; F2=&quot;/fastq_filtered/${SAMPLE_NAME}_R2_clean.fastq.gz&quot; echo &quot;BAM to FASTQ completed for $SAMPLE_NAME&quot; # Remove BAM files rm &quot;bam_files/${SAMPLE_NAME}_unaligned.bam&quot; rm &quot;bam_files/${SAMPLE_NAME}_deduplicated.bam&quot; echo &quot;Processing completed for $SAMPLE_NAME!&quot; Output: All and duplication metrics (*_duplicate_metrics.txt) were saved to the bam_files/ directory. The intermediate BAM files were removed to save memory. No major PCR duplication was observed. According to Picard, most samples had zero or negligible duplicated reads, meaning the original RNA-seq libraries were solid. Note: All scripts used in this section are stored in the scripts/ directory as 02_runFastqc.sbatch, and 03_runDeduplication.sbatch. "],["pseudosamples-from-raw-reads.html", "4 Pseudosamples from Raw Reads 4.1 Merge Paired-end Reads into a Single FASTA 4.2 Split into Pseudosamples", " 4 Pseudosamples from Raw Reads The original dataset included only two biological replicates per treatment, which is definitely not enough for training machine learning models. To make the most of the data, I implemented a pseudosample strategy: splitting each sample into 100 non-overlapping subsets while keeping the paired-end structure intact. These pseudosamples aren’t biologically independent, but they allow the model to “see” more of the internal structure and variability while still respecting the original replicate boundaries during validation. 4.1 Merge Paired-end Reads into a Single FASTA Before splitting, forward and reverse reads were merged into a single interleaved FASTA file. This made downstream processing cleaner and kept both ends of each read together. #!/bin/bash -l #SBATCH --job-name=data920 #SBATCH --time=4-00:00:00 #SBATCH --mem=50G #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --array=1-14 FASTQ_LIST=($(ls ./fasta_files/*_1.fasta)) FASTQ1=${FASTQ_LIST[$SLURM_ARRAY_TASK_ID - 1]} FASTQ2=$(echo $FASTQ1 | sed &#39;s/_1.fasta/_2.fasta/&#39;) OUTPUT=&quot;$(basename &quot;$FASTQ1&quot; | sed &#39;s/_1.fasta/_merged.fasta/&#39;)&quot; cat &quot;$FASTQ1&quot; &quot;$FASTQ2&quot; &gt; &quot;$OUTPUT&quot; echo &quot;Done for $FASTQ1 e $FASTQ2!&quot; 4.2 Split into Pseudosamples Each merged FASTA file was then split into 100 pseudosamples in a seven-step routine that’s part bash wizardry, part bioinformatics therapy: Step 1: Extract unique base read IDs Pulled read IDs from the merged FASTA and removed /1 and /2 suffixes to match up read pairs. The goal: treat each pair as a single unit. Step 2: Shuffle Shuffled the read ID list to keep things unbiased. Step 3: Split into 100 pseudo-ID files Divided the shuffled list into 100 chunks. Each one would become a pseudosample. Step 4: Generate paired-end ID files Reattached the /1 and /2 to make sure paired reads were preserved. Step 5: Create new pseudo-FASTA files Filtered the original merged FASTA to extract only the reads corresponding to each pseudo-ID file. Result: one FASTA per pseudosample. Step 6: Create summary file For each original sample, a table was created listing how many reads ended up in each pseudosample and what their IDs were. Future-me says thanks. Step 7: Clean up intermediate files Deleted the temporary ID files. They served their purpose. #!/bin/bash #SBATCH --job-name=pseudo_split #SBATCH --time=1-00:00:00 #SBATCH --array=0-13 #SBATCH --mem=25G #SBATCH --cpus-per-task=1 echo &quot;Job started on $(date)&quot; FASTA_LIST=($(ls fasta_files/*_merged.fasta)) FASTA=&quot;${FASTA_LIST[$SLURM_ARRAY_TASK_ID]}&quot; BASENAME=$(basename &quot;$FASTA&quot; _merged.fasta) echo &quot;Processing: $BASENAME&quot; mkdir -p ps_fasta/${BASENAME}/ids # Step 1: Extract unique base read IDs echo &quot;Extracting read IDs...&quot; grep &quot;&gt;&quot; &quot;$FASTA&quot; | sed &#39;s/&gt;//g&#39; | sed &#39;s/\\/[12]$//&#39; | sort -u &gt; ps_fasta/${BASENAME}/ids/${BASENAME}_ID.txt # Step 2: Shuffle echo &quot;Shuffling IDs...&quot; shuf ps_fasta/${BASENAME}/ids/${BASENAME}_ID.txt &gt; ps_fasta/${BASENAME}/ids/${BASENAME}_shufID.txt # Step 3: Split into 100 pseudo-ID files echo &quot;Splitting into 100 pseudo-ID files...&quot; total=$(wc -l &lt; ps_fasta/${BASENAME}/ids/${BASENAME}_shufID.txt) lines_per_file=$(( (total + 99) / 100 )) split -l $lines_per_file -d -a 2 ps_fasta/${BASENAME}/ids/${BASENAME}_shufID.txt ps_fasta/${BASENAME}/ids/pseudo_ # Step 4: Generate paired-end ID files echo &quot;Creating paired-end ID files...&quot; for f in ps_fasta/${BASENAME}/ids/pseudo_*; do awk &#39;{print $0&quot;/1\\n&quot;$0&quot;/2&quot;}&#39; &quot;$f&quot; &gt; &quot;${f}.txt&quot; done # Step 5: Create new pseudo-fasta files echo &quot;Generating pseudo-fasta files...&quot; for i in $(seq -w 0 99); do seqkit grep -f ps_fasta/${BASENAME}/ids/pseudo_${i}.txt &quot;$FASTA&quot; -o ps_fasta/${BASENAME}/${BASENAME}_ps${i}.fasta echo &quot;${BASENAME}_ps${i}.fasta done.&quot; done # Step 6: Create summary file echo &quot;Creating summary...&quot; SUMMARY_FILE=&quot;ps_fasta/${BASENAME}/pseudo_summary.txt&quot; &gt; &quot;$SUMMARY_FILE&quot; for f in ps_fasta/${BASENAME}/ids/pseudo_*; do IDFILE=$(basename &quot;$f&quot;) GROUP=&quot;${IDFILE##*_}&quot; GROUP=&quot;${GROUP/.txt/}&quot; clean_ids=$(awk -F&#39;/&#39; &#39;{print $1}&#39; &quot;$f&quot; | sort -u) count=$(echo &quot;$clean_ids&quot; | wc -l) ids_line=$(echo &quot;$clean_ids&quot; | paste -sd &quot;,&quot; -) echo &quot;&gt; pseudo_${GROUP} (${count} reads)&quot; &gt;&gt; &quot;$SUMMARY_FILE&quot; echo &quot;$ids_line&quot; &gt;&gt; &quot;$SUMMARY_FILE&quot; done # Step 7: Clean up intermediate pseudo ID files echo &quot;Cleaning up intermediate ID files...&quot; rm ps_fasta/${BASENAME}/ids/pseudo_* echo &quot;All pseudo-samples for $BASENAME created successfully on $(date)&quot; In total, this produced 1,400 pseudosamples (14 original samples \\(\\times\\) 100 splits), which were then used as input for k-mer decomposition and model training in the next phases. Note: To keep memory usage under control, all pseudosample FASTA files and summary tables were compressed into pseudoFastas.tar.xz. Unpack only if you’re ready. The scripts used to run this whole routine are in the scripts/ directory. "],["k-mer-extraction-and-filtering.html", "5 K-mer Extraction and Filtering 5.1 K-mer Counting with Jellyfish 5.2 Pseudosample Merging by Treatment 5.3 Global Merging by Group 5.4 Aggregation and Total Count per K-mer 5.5 Filtering kmers", " 5 K-mer Extraction and Filtering To quantify sequence patterns across pseudo-samples without depending on a reference genome, I extracted 41-mers directly from raw reads using Jellyfish. This section outlines the full process of counting, filtering, and condensing millions of k-mers into something the models could actually handle. 5.1 K-mer Counting with Jellyfish Jellyfish v2.3.0 was used to count canonical 41-mers from each pseudosample FASTA file. To avoid getting distracted by sequencing noise, only k-mers with frequency &gt;= 10 were kept. The pipeline used a two-pass system: A filter was used to identify promising k-mer candidates, followed by a targeted recount of those that appeared frequently. #!/bin/bash #SBATCH --job-name=kmer_count #SBATCH --time=7-00:00:00 #SBATCH --array=0-1399%100 #SBATCH --mem=25G #SBATCH --cpus-per-task=10 module load Jellyfish/2.3.0-GCC-11.3.0 echo &quot;Job started on $(date)&quot; # Step 1: Generate a list of all pseudo-fasta files if [ &quot;$SLURM_ARRAY_TASK_ID&quot; -eq 0 ]; then echo &quot;Generating list of pseudo-fasta files...&quot; find ps_fasta -type f -name &quot;*_ps*.fasta&quot; | sort &gt; all_pseudo_fastas.list fi sleep 10 # Step 2: file path FASTA=$(sed -n &quot;$((SLURM_ARRAY_TASK_ID + 1))p&quot; all_pseudo_fastas.list) BASENAME=$(basename &quot;$FASTA&quot; .fasta) DIRNAME=$(dirname &quot;$FASTA&quot;) echo &quot;Processing: $FASTA&quot; # Step 3: Define output file paths BCFILE=&quot;${DIRNAME}/${BASENAME}.bc&quot; JFFILE=&quot;${DIRNAME}/${BASENAME}.jf&quot; TSVFILE=&quot;${DIRNAME}/${BASENAME}_k41.tsv&quot; # Step 4: First pass - Bloom Counter jellyfish bc -m 41 -s 300M -t 10 -o &quot;$BCFILE&quot; &quot;$FASTA&quot; # Step 5: Second pass - count only k-mers with frequency &gt;= 2 jellyfish count -m 41 -s 300M -t 10 --bc &quot;$BCFILE&quot; -C -o &quot;$JFFILE&quot; &quot;$FASTA&quot; # Step 6: TSV format -&gt; transformer jellyfish dump -c &quot;$JFFILE&quot; &gt; &quot;$TSVFILE&quot; echo &quot;Finished: $TSVFILE&quot; # Step 7: Rovome temporary Jellyfish files rm -f &quot;$BCFILE&quot; &quot;$JFFILE&quot; echo &quot;Temporary files removed: $BCFILE and $JFFILE&quot; Output: One .tsv file per pseudosample containing k-mer sequences and their counts. Temporary .jf and .bc files were deleted to conserve space. The .tsv were compressed into the file count.tar.xz. 5.2 Pseudosample Merging by Treatment Given the total number of pseudosamples, processing them all at once wasn’t an option (R crashed several times, no matter how much RAM I added) in real time. So, I merged count files in batches of 10 per treatment using csvtk, generating manageable chunks like SRR1542404_00-09.tsv. #!/bin/bash #SBATCH --job-name=merge_kmers #SBATCH --array=0-13 #SBATCH --mem=200G #SBATCH --time=12:00:00 #SBATCH --cpus-per-task=15 # List of treatment IDs (folders) TREAT_IDS=(SRR1542404 SRR1542405 SRR1542406 SRR1542407 SRR1542408 SRR1542409 SRR1542410 SRR1542411 SRR1542412 SRR1542413 SRR1542414 SRR1542415 SRR1542416 SRR1542417) TREAT_ID=${TREAT_IDS[$SLURM_ARRAY_TASK_ID]} echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] Treatment start: $TREAT_ID&quot; cd $TREAT_ID # Step 1: Fix TSV files echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] Fixing TSV files.&quot; for f in *.tsv; do #insert header before the original first line sed -i &#39;1s/^/kmer\\tcount\\n/&#39; &quot;$f&quot; #After line 2, replace first space with tab sed -i &#39;2,$s/ /\\t/&#39; &quot;$f&quot; done # Step 2: Merge all pseudo‐samples ps00…ps99 into a single matrix echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 00-09.&quot; ~/Tools_Scripts/csvtk join *_ps{00..09}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_00-09.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 10-19.&quot; ~/Tools_Scripts/csvtk join *_ps{10..19}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_10-19.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 20-29.&quot; ~/Tools_Scripts/csvtk join *_ps{20..29}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_20-29.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 30-39.&quot; ~/Tools_Scripts/csvtk join *_ps{30..39}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_30-39.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 40-49.&quot; ~/Tools_Scripts/csvtk join *_ps{40..49}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_40-49.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 50-59.&quot; ~/Tools_Scripts/csvtk join *_ps{50..59}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_50-59.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 60-69.&quot; ~/Tools_Scripts/csvtk join *_ps{60..69}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_60-69.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 70-79.&quot; ~/Tools_Scripts/csvtk join *_ps{70..79}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_70-79.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 80-89.&quot; ~/Tools_Scripts/csvtk join *_ps{80..89}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_80-89.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] 90-99.&quot; ~/Tools_Scripts/csvtk join *_ps{90..99}_k41.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; ../${TREAT_ID}_90-99.tsv # Done echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] $TREAT_ID completed.&quot; Note: Each treatment resulted in 10 merged files, stored as TREAT_ID_00-09.tsv to TREAT_ID_90-99.tsv. 5.3 Global Merging by Group Even after merging by treatment, the data was still too big for a single R session (or for any mortal RAM configuration under 300 GB). So, treatments were split into two groups, each processed independently. For each group, the 10 treatment files were combined into 10 final matrices named like FINAL_group1_PART.tsv. #!/bin/bash #SBATCH --job-name=merge_kmers #SBATCH --array=0-9 #SBATCH --mem=200G #SBATCH --time=12:00:00 #SBATCH --cpus-per-task=15 PARTS=(_00-09.tsv _10-19.tsv _20-29.tsv _30-39.tsv _40-49.tsv _50-59.tsv _60-69.tsv _70-79.tsv _80-89.tsv _90-99.tsv) PART=${PARTS[$SLURM_ARRAY_TASK_ID]} echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 1&quot; ~/Tools_Scripts/csvtk join &quot;SRR1542404$PART&quot; &quot;SRR1542407$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 2&quot; ~/Tools_Scripts/csvtk join &quot;final$PART&quot; &quot;SRR1542409$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final2$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 3&quot; ~/Tools_Scripts/csvtk join &quot;final2$PART&quot; &quot;SRR1542411$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 4&quot; ~/Tools_Scripts/csvtk join &quot;final$PART&quot; &quot;SRR1542413$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final2$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 5&quot; ~/Tools_Scripts/csvtk join &quot;final2$PART&quot; &quot;SRR1542415$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 6&quot; ~/Tools_Scripts/csvtk join &quot;final$PART&quot; &quot;SRR1542416$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final2$PART&quot; mv &quot;final2$PART&quot; &quot;FINAL_group1$PART&quot; echo &quot;-----Group 2------&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 1&quot; ~/Tools_Scripts/csvtk join &quot;SRR1542405$PART&quot; &quot;SRR1542406$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 2&quot; ~/Tools_Scripts/csvtk join &quot;final$PART&quot; &quot;SRR1542408$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final2$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 3&quot; ~/Tools_Scripts/csvtk join &quot;final2$PART&quot; &quot;SRR1542410$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 4&quot; ~/Tools_Scripts/csvtk join &quot;final$PART&quot; &quot;SRR1542412$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final2$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 5&quot; ~/Tools_Scripts/csvtk join &quot;final2$PART&quot; &quot;SRR1542414$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final$PART&quot; echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 6&quot; ~/Tools_Scripts/csvtk join &quot;final$PART&quot; &quot;SRR1542417$PART&quot; -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; &quot;final2$PART&quot; mv &quot;final2$PART&quot; &quot;FINAL_group2$PART&quot; Output: A total of 20 files (10 per group), later converted to .rds format for faster loading in R. The original .tsv files were deleted, and the .rds versions were bundled in kmer_count.tar.xz. 5.4 Aggregation and Total Count per K-mer Each final matrix was loaded in R, and total counts per k-mer were computed across all pseudosamples. A .tsv version of the totals was created for external filtering, while the intermediate .rds files were kept for sanity (and speed). Note: All this was done via SLURM jobs, a necessary evil that saved me time. #!/bin/bash #SBATCH --job-name=kmers #SBATCH --array=0-19 #SBATCH --time=04:00:00 #SBATCH --mem=50G #SBATCH --cpus-per-task=1 module load R/4.2.1-foss-2022a FILES_LIST=($(ls FINAL_group*.tsv)) FILE=&quot;${FILES_LIST[$SLURM_ARRAY_TASK_ID]}&quot; R --no-save -q &lt; editFiles1.R &quot;$FILE&quot; editFiles1.R #!/usr/bin/env Rscript args &lt;- commandArgs(trailingOnly = TRUE) print(args[1]) path_files &lt;- args[1] library(data.table) library(stringr) setwd(&#39;/bulk/eakhunov/grcampos/embeddings/&#39;) tmp_mat &lt;- fread(paste0(&#39;/bulk/eakhunov/grcampos/embeddings/kmer_count/&#39;, path_files)) colnames(tmp_mat)[-1] &lt;- str_extract(colnames(tmp_mat)[-1], &quot;SRR\\\\d+_ps\\\\d+&quot;) kmer_check &lt;- data.frame(kmer = tmp_mat$kmer, count = rowSums(tmp_mat[, -1, with = FALSE])) name_base &lt;- tools::file_path_sans_ext(basename(path_files)) saveRDS(tmp_mat, file = paste0(&#39;/bulk/eakhunov/grcampos/embeddings/kmer_count/&#39;, name_base, &#39;.rds&#39;)) write.table(kmer_check, file = paste0(&#39;/bulk/eakhunov/grcampos/embeddings/kmer_count/count&#39;, name_base, &#39;.tsv&#39;), quote = F, row.names = F, sep = &#39;\\t&#39;) print(&#39;Done!&#39;) 5.5 Filtering kmers 5.5.1 Step 1: Abundance Threshold First, I filtered out k-mers with total count &lt; 10, reducing the dataset from a dropping ~88 million k-mers to a more reasonable ~45 million. This required recombining all parts into a single matrix (a fun reminder that splitting things helps performance, but reuniting them is unavoidable in the end). Matrix rows were reordered to align across all files, for consistency across files. #!/bin/bash #SBATCH --job-name=merge_count2 #SBATCH --array=0 #SBATCH --mem=200G #SBATCH --time=12:00:00 #SBATCH --cpus-per-task=15 echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 1&quot; ~/Tools_Scripts/csvtk join count_00-09.tsv count_10-19.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 2&quot; ~/Tools_Scripts/csvtk join FINAL.tsv count_20-39.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL2.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 3&quot; ~/Tools_Scripts/csvtk join FINAL2.tsv count_30-39.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 4&quot; ~/Tools_Scripts/csvtk join FINAL.tsv count_40-49.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL2.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 5&quot; ~/Tools_Scripts/csvtk join FINAL2.tsv count_50-59.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 6&quot; ~/Tools_Scripts/csvtk join FINAL.tsv count_60-69.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL2.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 7&quot; ~/Tools_Scripts/csvtk join FINAL2.tsv count_70-79.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 8&quot; ~/Tools_Scripts/csvtk join FINAL.tsv count_80-89.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL2.tsv echo &quot;[$(date &#39;+%Y-%m-%d %H:%M:%S&#39;)] step 9&quot; ~/Tools_Scripts/csvtk join FINAL2.tsv count_90-99.tsv -t -f 1 -j 15 --outer-join --na 0 --prefix-filename --prefix-trim-ext &gt; FINAL.tsv mv FINAL.tsv kmer_count.tsv rm FINAL2.tsv module load R/4.2.1-foss-2022a sbatch --job-name=R --time=01-0:00:00 --mem=100G --nodes=1 --wrap=&quot;R --no-save -q &lt; editFiles2.R&quot; editFiles2.R #!/usr/bin/env Rscript library(data.table) kmer_count &lt;- fread(&#39;kmer_count/kmer_count.tsv&#39;) %&gt;% rename_with(~ str_extract(., &quot;group[12](?:_\\\\d{2}-\\\\d{2})?&quot;), -1) %&gt;% mutate(TOTAL = rowSums(across(-1))) %&gt;% filter(TOTAL &gt;= 10) %&gt;% select(1, TOTAL) #45M saveRDS(as.vector(kmer_filterMin$kmer), file = &#39;kmer_count/kmer_filterMin.rds&#39;) #!/bin/bash #SBATCH --job-name=kmers #SBATCH --array=0-19 #SBATCH --time=04:00:00 #SBATCH --mem=200G #SBATCH --cpus-per-task=1 module load R/4.2.1-foss-2022a FILES_LIST=($(ls FINAL_group2*.rds)) FILE=&quot;${FILES_LIST[$SLURM_ARRAY_TASK_ID]}&quot; Rscript editFiles3.R &quot;$FILE&quot; editFiles3.R #!/usr/bin/env Rscript args &lt;- commandArgs(trailingOnly = TRUE) print(args[1]) path_files &lt;- args[1] library(data.table) library(stringr) library(tools) setwd(&#39;/bulk/eakhunov/grcampos/embeddings/&#39;) tmp_dt &lt;- readRDS(paste0(&#39;kmer_count/&#39;, path_files)) kmers &lt;- readRDS(&#39;kmer_count/kmer_filterMin.rds&#39;) name_base &lt;- file_path_sans_ext(basename(path_files)) cat(&quot;Min filter\\n&quot;) tmp_dt &lt;- tmp_dt[kmer %in% kmers] cat(&quot;Missing kmers\\n&quot;) missing_kmers &lt;- setdiff(kmers, tmp_dt$kmer) if (length(missing_kmers) &gt; 0) { n_cols &lt;- ncol(tmp_dt) - 1 tmp_missing &lt;- data.table(kmer = missing_kmers, matrix(0, nrow = length(missing_kmers), ncol = n_cols)) setnames(tmp_missing, colnames(tmp_dt)) tmp_dt &lt;- rbind(tmp_dt, tmp_missing) } gc() cat(&quot;Ordering kmers\\n&quot;) tmp_dt &lt;- tmp_dt[match(kmers, tmp_dt$kmer)] cat(&quot;Saving\\n&quot;) saveRDS(tmp_dt, file = paste0(&#39;kmer_count/&#39;, name_base, &#39;_filter_min.rds&#39;)) cat(&quot;Done!\\n&quot;) 5.5.2 Step 2: Coefficient of Variation (CV) Next, I wanted to keep only the most dynamic k-mers, so I calculated the coefficient of variation (CV) for each. This task was divided into ~4,000 chunks and submitted as SLURM jobs. Runtime? 48 hours. #!/bin/bash #SBATCH --job-name=chunk_cv #SBATCH --array=374-4505%100 #SBATCH --time=2-00:00:00 #SBATCH --mem=50G #SBATCH --cpus-per-task=1 module load R/4.2.1-foss-2022a Rscript kmers_CV_byChunck.R $SLURM_ARRAY_TASK_ID kmers_CV_byChunck.R #!/usr/bin/env Rscript args &lt;- commandArgs(trailingOnly = TRUE) chunk_id &lt;- as.integer(args[1]) library(data.table) setwd(&#39;/bulk/eakhunov/grcampos/embeddings/&#39;) files &lt;- list.files(&quot;kmer_count/&quot;, pattern = &quot;_filter_min.rds$&quot;, full.names = TRUE) chunk_size &lt;- 10000 n_total &lt;- nrow(readRDS(files[1])) rowSds &lt;- function(x) sqrt(rowMeans((x - rowMeans(x))^2)) line_start &lt;- (chunk_id - 1) * chunk_size + 1 line_end &lt;- min(chunk_id * chunk_size, n_total) cat(&quot;Processing chunk&quot;, chunk_id, &quot;:&quot;, line_start, &quot;-&quot;, line_end, &quot;\\n&quot;) all_cols &lt;- vector(&quot;list&quot;, length(files)) kmer_ref &lt;- NULL for (j in seq_along(files)) { dt &lt;- readRDS(files[j])[line_start:line_end] if (j == 1) kmer_ref &lt;- dt$kmer all_cols[[j]] &lt;- dt[, -1, with = FALSE] } mat &lt;- do.call(cbind, all_cols) media &lt;- rowMeans(mat) desvio &lt;- rowSds(mat) cv &lt;- desvio / media result &lt;- data.table(kmer = kmer_ref, mean = media, sd = desvio, cv = cv) # saving out_path &lt;- sprintf(&quot;kmer_count/chunks_cv/chunk_%04d.rds&quot;, chunk_id) dir.create(&quot;kmer_count/chunks_cv&quot;, showWarnings = FALSE) saveRDS(result, out_path) cat(&quot;Chunk&quot;, chunk_id, &quot;saved to&quot;, out_path, &quot;\\n&quot;) Output: All files are available in the folder CVbyChunck.tar.xz. After processing all chunks, results were combined and kmers in the top 25% of CV were retained. module load R/4.2.1-foss-2022a sbatch --job-name=R --time=01-0:00:00 --mem=100G --nodes=1 --wrap=&quot;R --no-save -q &lt; editFiles4.R&quot; editFiles4.R #!/usr/bin/env Rscript library(data.table) library(tidyverse) file_paths &lt;- paste0(&#39;kmer_count/chunks_cv/&#39;, list.files(&#39;kmer_count/chunks_cv/&#39;)) kmer_cv &lt;- lapply(file_paths, readRDS) kmer_cv &lt;- do.call(rbind, kmer_cv) table(kmer_count$kmer %in% kmer_cv$kmer) kmer_metrics &lt;- left_join(kmer_cv, kmer_count, by = &quot;kmer&quot;) # top 25% fasta_lines &lt;- kmer_metrics %&gt;% filter(cv &gt; quantile(cv, 0.75)) %&gt;% arrange(desc(cv), desc(TOTAL)) %&gt;% select(-mean, -sd) fasta_lines &lt;- paste0(&quot;&gt;kmer&quot;, seq_len(nrow(kmer_metrics)), &quot;\\n&quot;, kmer_metrics$kmer) write_lines(fasta_lines, &quot;kmers_min_cv.fa&quot;) Note: The resulting file kmers_min_cv.fa contains the most variable and abundant k-mers for cd-hit-est clustering. 5.5.3 Step 3: Redundancy Removal with CD-HIT-EST Finally, to avoid feeding the models with 10 slightly different versions of the same sequence, I ran CD-HIT-EST with a 95% identity threshold. Only one representative per cluster was kept. module load CD-HIT/4.8.1-GCC-11.3.0 sbatch --job-name=CdH --time=05:00:00 --mem=200G --ntasks-per-node=20 --nodes=1 --wrap=&quot;cd-hit-est -i kmers_min_cv.fa -o kmers_clustered.fa -c 0.95 -n 8 -T 20 -M 200000 -d 0&quot; Output: Clustering results and retained sequences are in the cd-hit.tar.xz folder. module load R/4.2.1-foss-2022a sbatch --job-name=R --time=01-0:00:00 --mem=100G --nodes=1 --wrap=&quot;R --no-save -q &lt; editFiles5.R&quot; editFiles5.R #!/usr/bin/env Rscript library(data.table) library(tidyverse) library(Biostrings) cdhit &lt;- readDNAStringSet(&quot;kmers_clustered.fa&quot;) seqs &lt;- as.character(cdhit) head(seqs) length(seqs) kmer_metrics &lt;- filter(kmer_metrics, kmer %in% seqs) #4.5M head(kmer_metrics) hist(kmer_metrics$cv) # count matrices ---- path_count &lt;- list.files(&quot;kmer_count&quot;, pattern = &quot;_filter_min.rds$&quot;, full.names = TRUE) count_clean &lt;- data.table() for(i in seq_along(path_count)){ #4h tmp &lt;- readRDS(path_count[i]) %&gt;% filter(kmer %in% seqs2) %&gt;% arrange(factor(kmer, levels = seqs)) %&gt;% column_to_rownames(var = &#39;kmer&#39;) count_clean &lt;- cbind(count_clean, tmp) } rownames(count_clean) &lt;- seqs kmer_metrics_grouped &lt;- kmer_metrics %&gt;% group_by(mean, sd, TOTAL, cv) %&gt;% mutate(group_id = cur_group_id()) %&gt;% ungroup() %&gt;% as.data.frame() kmer_unique &lt;- kmer_metrics_grouped[!duplicated(kmer_metrics_grouped$group_id), ] count_clean2 &lt;- count_clean %&gt;% rownames_to_column(&quot;kmer&quot;) %&gt;% filter(kmer %in% kmer_unique$kmer) %&gt;% column_to_rownames(&quot;kmer&quot;) count_clean2 &lt;- log2(count_clean2 + 1) count_clean2 &lt;- t(count_clean2) saveRDS(count_clean, &#39;results/count_clean.rds&#39;) saveRDS(kmer_metrics, &#39;results/kmer_metrics.rds&#39;) saveRDS(count_clean2, &#39;results/count_clean2.rds&#39;) All scripts used in this straightforward chain of events are available in the scripts/ directory, for anyone brave (or curious) enough to follow the path. "],["exploratory-analysis.html", "6 Exploratory Analysis 6.1 Total counts 6.2 Variance 6.3 PCA 6.4 t-SNE", " 6 Exploratory Analysis To explore the structure and quality of the filtered k-mer count matrix, I ran a few exploratory analyses to understand the structure and quality of the filtered k-mer count matrix. These steps help expose hidden patterns across pseudo-samples, flag weird samples, and check whether the data even has a chance of being separable in a lower-dimensional space. mat &lt;- readRDS(&#39;~/AGRO920/results/count_clean2.rds&#39;) treat_map &lt;- c(SRR1542404 = &quot;Control&quot;, SRR1542405 = &quot;Control&quot;, SRR1542406 = &quot;Drought_1h&quot;, SRR1542407 = &quot;Drought_1h&quot;, SRR1542408 = &quot;Drought_6h&quot;, SRR1542409 = &quot;Drought_6h&quot;, SRR1542410 = &quot;Heat_1h&quot;, SRR1542411 = &quot;Heat_1h&quot;, SRR1542412 = &quot;Heat_6h&quot;, SRR1542413 = &quot;Heat_6h&quot;, SRR1542414 = &quot;Drought_Heat_1h&quot;,SRR1542415 = &quot;Drought_Heat_1h&quot;, SRR1542416 = &quot;Drought_Heat_6h&quot;, SRR1542417 = &quot;Drought_Heat_6h&quot;) y &lt;- data.frame(sample = sub(&quot;_.*&quot;, &quot;&quot;, rownames(mat)), ps = rownames(mat)) y$treat &lt;- factor(treat_map[y$sample]) 6.1 Total counts I calculated the total k-mer count for each pseudosample and visualized it by sample and treatment. library(tidyverse) total_counts &lt;- rowSums(mat) variances &lt;- apply(mat, 1, var) df_summary &lt;- data.frame(Pseudosample = rownames(mat), Sample = sub(&quot;_.*&quot;, &quot;&quot;, rownames(mat)), Total = total_counts, Variance = variances) df_summary$treat &lt;- factor(treat_map[df_summary$Sample]) ggplot(df_summary, aes(Sample, Total, color = treat, fill = treat)) + geom_boxplot(alpha = 0.5) + theme_light() + labs(title = &quot;Total k-mer Counts per Pseudosample&quot;, y = &quot;Total Counts&quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + scale_fill_brewer(palette = &quot;Set2&quot;) + scale_color_brewer(palette = &quot;Set2&quot;) Most treatments behaved nicely, showing relatively consistent totals across replicates. Then came control sample SRR1542404, with noticeably higher counts and more variability than anyone asked for. This could be due to differences in sequencing depth or pre-processing hiccups. The good news: no systematic bias was observed across the remaining samples, so the pipeline (mostly) survived its first test. 6.2 Variance To evaluate within treatment consistency, I looked at the variance of k-mer counts per pseudosample and visualized it by treatment and replicate. df_summary$rep &lt;- factor(c(rep(1, 100), rep(2,100))) ggplot(df_summary, aes(x = Variance, color = rep, fill = rep)) + geom_density(alpha = 0.5) + theme_light() + labs(title = &quot;Density of Variance per Treatment&quot;, x = &quot;Variance&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) + scale_color_brewer(palette = &quot;Set2&quot;) + theme(legend.title = element_blank()) + facet_wrap(~treat, scales = &#39;free&#39;) Overall, Drought_1h, Drought_Heat_1h, and Heat_1h showed small differences between replicates. This could be due to real biological variation or just some noise from sequencing. The control group had higher variation again, which makes sense because it also had more total reads. 6.3 PCA library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout pca &lt;- prcomp(mat, center = TRUE, scale. = TRUE) pca_df &lt;- as.data.frame(pca$x[, 1:3]) pca_df$treat &lt;- y$treat #head(pca_df) plot_ly(pca_df, x = ~PC1, y = ~PC2, z = ~PC3, color = ~treat, colors = &quot;Set2&quot;, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) %&gt;% layout(title = &quot;PCA: First 3 Components&quot;) The first three principal components explained… less than 1% of the variance. The PCA plot was essentially an abstract painting with no clear separation between treatment groups, no clusters, just vibes. This suggests that variation is spread thinly across a high number of dimensions and that PCA, while still a classic, isn’t going to solve this mystery alone. 6.4 t-SNE library(Rtsne) tsne_res &lt;- Rtsne(mat, dims = 2, perplexity = 5, verbose = TRUE, max_iter = 1000) ## Performing PCA ## Read the 1400 x 50 data matrix successfully! ## OpenMP is working. 1 threads. ## Using no_dims = 2, perplexity = 5.000000, and theta = 0.500000 ## Computing input similarities... ## Building tree... ## Done in 0.09 seconds (sparsity = 0.020113)! ## Learning embedding... ## Iteration 50: error is 88.560362 (50 iterations in 0.21 seconds) ## Iteration 100: error is 82.297847 (50 iterations in 0.12 seconds) ## Iteration 150: error is 81.909762 (50 iterations in 0.11 seconds) ## Iteration 200: error is 81.881094 (50 iterations in 0.12 seconds) ## Iteration 250: error is 81.861104 (50 iterations in 0.11 seconds) ## Iteration 300: error is 2.392276 (50 iterations in 0.10 seconds) ## Iteration 350: error is 2.011479 (50 iterations in 0.10 seconds) ## Iteration 400: error is 1.860224 (50 iterations in 0.11 seconds) ## Iteration 450: error is 1.794013 (50 iterations in 0.10 seconds) ## Iteration 500: error is 1.759940 (50 iterations in 0.10 seconds) ## Iteration 550: error is 1.735467 (50 iterations in 0.11 seconds) ## Iteration 600: error is 1.719964 (50 iterations in 0.11 seconds) ## Iteration 650: error is 1.705950 (50 iterations in 0.11 seconds) ## Iteration 700: error is 1.692623 (50 iterations in 0.11 seconds) ## Iteration 750: error is 1.684094 (50 iterations in 0.11 seconds) ## Iteration 800: error is 1.675477 (50 iterations in 0.11 seconds) ## Iteration 850: error is 1.669563 (50 iterations in 0.11 seconds) ## Iteration 900: error is 1.665564 (50 iterations in 0.10 seconds) ## Iteration 950: error is 1.661259 (50 iterations in 0.10 seconds) ## Iteration 1000: error is 1.657469 (50 iterations in 0.11 seconds) ## Fitting performed in 2.26 seconds. tsne_res_df &lt;- as.data.frame(tsne_res$Y) tsne_res_df$treat &lt;- y$treat plot_ly(tsne_res_df, x = ~V1, y = ~V2, color = ~treat, colors = &quot;Set2&quot;, type = &quot;scatter&quot;, mode = &quot;markers&quot;) Unlike PCA, t-SNE actually did something useful. It revealed structured groupings among treatments, with some overlap but still kept a bit of internal structure. Overall, the t-SNE results suggest that the k-mer-based representation does capture biologically meaningful differences between treatments, enough to justify moving forward with classification models. "],["models.html", "7 Models 7.1 Random Forest", " 7 Models 7.1 Random Forest To explore how well the data could be classified, I tested a grid of hyperparameter combinations, varying two core knobs of the Random Forest algorithm: mtry: Number of variables randomly sampled at each split (set to 500, 1000, and p — where p is the total number of predictors) nodesize: Minimum number of samples per terminal node (set to 5, 10, and 20) These values were chosen to explore the tradeoff between shallow vs deep trees and narrow vs wide feature subsets. The models were trained and evaluated using a one-vs-all strategy for four stress conditions. For each condition, one biological replicate was used for training and the other for testing because, as you may recall, I only had two replicates per condition and had to get creative. Thanks to the 13k+ predictors, 4 stress conditions, and 9 hyperparameter combos, training all models took around 20 hours on 30 parallel workers. Each Random Forest used 500 trees and applied class weights to reduce the classic “I’ll-just-predict-the-majority-class” laziness. #!/usr/bin/env Rscript cat(&quot;STARTING SCRIPT AT: &quot;, as.character(Sys.time()), &quot;\\n&quot;) ## Data ---- library(tidyverse) library(randomForest) library(doParallel) library(foreach) library(stringr) cat(&quot;Loading data...\\n&quot;) count_clean2 &lt;- readRDS(&#39;~/AGRO920/results/count_clean2.rds&#39;) treat_map &lt;- c(SRR1542404 = &quot;Control&quot;, SRR1542405 = &quot;Control&quot;, SRR1542406 = &quot;Drought_1h&quot;, SRR1542407 = &quot;Drought_1h&quot;, SRR1542408 = &quot;Drought_6h&quot;, SRR1542409 = &quot;Drought_6h&quot;, SRR1542410 = &quot;Heat_1h&quot;, SRR1542411 = &quot;Heat_1h&quot;, SRR1542412 = &quot;Heat_6h&quot;, SRR1542413 = &quot;Heat_6h&quot;, SRR1542414 = &quot;Drought_Heat_1h&quot;,SRR1542415 = &quot;Drought_Heat_1h&quot;, SRR1542416 = &quot;Drought_Heat_6h&quot;, SRR1542417 = &quot;Drought_Heat_6h&quot;) y &lt;- data.frame(sample = sub(&quot;_.*&quot;, &quot;&quot;, rownames(count_clean2)), ps = rownames(count_clean2)) y$treat &lt;- factor(treat_map[y$sample]) set.seed(2310) sort_data &lt;- data.frame(Sample = sort(unique(y$sample)), Rep = factor(1:2)) sort_data$Trat &lt;- treat_map[sort_data$Sample] group1 &lt;- sort_data %&gt;% group_by(Trat) %&gt;% slice_sample(n = 1) %&gt;% ungroup() y_train &lt;- filter(y, sample %in% group1$Sample) y_test &lt;- filter(y, !(sample %in% group1$Sample)) train_idx &lt;- rownames(count_clean2) %in% y_train$ps X_train &lt;- count_clean2[train_idx, , drop = FALSE] test_idx &lt;- rownames(count_clean2) %in% y_test$ps X_test &lt;- count_clean2[test_idx, , drop = FALSE] treatments &lt;- unique(y_train$treat) labels &lt;- c(&quot;drought_1h&quot;, &quot;drought_6h&quot;, &quot;heat_1h&quot;, &quot;heat_6h&quot;) y_multi &lt;- y[, 1:3] %&gt;% mutate(drought_1h = ifelse(str_detect(treat, &quot;Drought&quot;) &amp; str_detect(treat, &quot;1h&quot;), 1, 0), drought_6h = ifelse(str_detect(treat, &quot;Drought&quot;) &amp; str_detect(treat, &quot;6h&quot;), 1, 0), heat_1h = ifelse(str_detect(treat, &quot;Heat&quot;) &amp; str_detect(treat, &quot;1h&quot;), 1, 0), heat_6h = ifelse(str_detect(treat, &quot;Heat&quot;) &amp; str_detect(treat, &quot;6h&quot;), 1, 0)) y_train_multi &lt;- y_multi %&gt;% filter(sample %in% group1$Sample) %&gt;% select(-sample, -treat) y_test_multi &lt;- y_multi %&gt;% filter(!(sample %in% group1$Sample)) %&gt;% select(-sample, -treat) X_train &lt;- as.data.frame(X_train) X_train$control &lt;- ifelse(y_train$treat == &quot;Control&quot;, 1, 0) X_test &lt;- as.data.frame(X_test) X_test$control &lt;- ifelse(y_test$treat == &quot;Control&quot;, 1, 0) ## Training ---- cat(&quot;TRAINING MODELS - START: &quot;, as.character(Sys.time()), &quot;\\n&quot;) cl &lt;- makeCluster(30) registerDoParallel(cl) mtry_vals &lt;- c(500, 1000, ncol(X_train)) nodesize_vals &lt;- c(5, 10, 20) grid &lt;- expand.grid(mtry = mtry_vals, nodesize = nodesize_vals) results_grid &lt;- list() for (label in labels) { results_grid[[label]] &lt;- foreach(i = 1:nrow(grid), .packages = &quot;randomForest&quot;) %dopar% { m &lt;- grid$mtry[i] n &lt;- grid$nodesize[i] result_model_list &lt;- list() for (t in treatments) { idx_test &lt;- which(y_train$treat == t) idx_train &lt;- which(y_train$treat != t) X_tmp &lt;- as.data.frame(X_train[idx_train, ]) X_tmp$control &lt;- ifelse(y_train$treat[idx_train] == &quot;Control&quot;, 1, 0) y_tmp &lt;- factor(y_train_multi[[label]][idx_train]) rf_model &lt;- randomForest(x = X_tmp, y = y_tmp, ntree = 500, mtry = m, nodesize = n, importance = TRUE, classwt = c(&quot;0&quot; = 1, &quot;1&quot; = 2.5)) pred_prob &lt;- predict(rf_model, newdata = cbind(X_train[idx_test, ], control = ifelse(y_train$treat[idx_test] == &quot;Control&quot;, 1, 0)),type = &quot;prob&quot;)[, 2] pred &lt;- ifelse(pred_prob &gt; 0.5, 1, 0) obs &lt;- y_train_multi[[label]][idx_test] result_model_list[[t]] &lt;- data.frame(treatment = t, predicted = pred, prob = pred_prob, observed = obs, mtry = m, nodesize = n) } do.call(rbind, result_model_list) } } cat(&quot;TRAINING MODELS - END: &quot;, as.character(Sys.time()), &quot;\\n&quot;) ## Test ---- cat(&quot;TESTING MODELS - START: &quot;, as.character(Sys.time()), &quot;\\n&quot;) rf_test_models &lt;- list() for (label in labels) { y_train_label &lt;- factor(y_train_multi[[label]]) y_test_label &lt;- factor(y_test_multi[[label]]) rf_test_models[[label]] &lt;- foreach(i = 1:nrow(grid), .packages = &quot;randomForest&quot;) %dopar% { m &lt;- grid$mtry[i] n &lt;- grid$nodesize[i] rf_model &lt;- randomForest(x = X_train, y = y_train_label, ntree = 500, mtry = m, nodesize = n, importance = TRUE, classwt = c(&quot;0&quot; = 1, &quot;1&quot; = 2.5)) pred_prob &lt;- predict(rf_model, X_test, type = &quot;prob&quot;)[,2] pred &lt;- ifelse(pred_prob &gt; 0.5, 1, 0) acc &lt;- mean(pred == y_test_label) list(model = rf_model, mtry = m, nodesize = n, acc = acc, predictions = pred, predicted_prob = pred_prob, observed = y_test_label) } } stopCluster(cl) registerDoSEQ() cat(&quot;TESTING MODELS - END: &quot;, as.character(Sys.time()), &quot;\\n&quot;) ## Saving ---- RF_final &lt;- list(training = results_grid, test = rf_test_models) saveRDS(RF_final, &#39;~/AGRO920/results/RF_final.rds&#39;) cat(&quot;SCRIPT FINISHED AT: &quot;, as.character(Sys.time()), &quot;\\n&quot;) library(caret) library(purrr) RF_final &lt;- readRDS(&quot;~/AGRO920/results/RF_final.rds&quot;) 7.1.1 Confusion Matrices conf_mats &lt;- list() for (label in names(RF_final$test)) { conf_mats[[label]] &lt;- list() for (model in RF_final$test[[label]]) { obs &lt;- model$observed pred &lt;- model$predictions key &lt;- paste0(&quot;mtry=&quot;, model$mtry, &quot;_node=&quot;, model$nodesize) cm &lt;- table(factor(obs, levels = c(0,1)), factor(pred, levels = c(0,1))) cm_prop &lt;- prop.table(cm, margin = 1) conf_mats[[label]][[key]] &lt;- cm_prop } } conf_df &lt;- map_dfr(names(conf_mats), function(label) { map_dfr(names(conf_mats[[label]]), function(model_id) { mat &lt;- conf_mats[[label]][[model_id]] mat %&gt;% as.data.frame(as.table(mat)) %&gt;% rename(True = Var1, Pred = Var2, Proportion = Freq) %&gt;% mutate(Label = label, Model = model_id) }) }) rownames(conf_df) &lt;- NULL ggplot(conf_df, aes(x = Pred, y = True, fill = Proportion)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;#08519c&quot;) + facet_grid(Label ~ Model) + geom_text(aes(label = round(Proportion, 2)), color = &quot;black&quot;, size = 3) + labs(x = &quot;Predicted Class&quot;, y = &quot;True Class&quot;, fill = &quot;Proportion&quot;) + theme_light(base_size = 12) + theme(strip.text = element_text(size = 10), axis.text = element_text(size = 10), axis.title = element_text(size = 11), panel.grid = element_blank(), strip.text.x = element_text(color = &#39;black&#39;), strip.text.y = element_text(color = &#39;black&#39;)) Only heat_1h behaved like a as a desired model: consistent, predictable, and easy to classify. The other conditions, especially anything drought-related, turned out to be more… dramatic. In many cases, models predicted everything as positive. That’s great for recall, but not so great for precision or balanced accuracy. Basically: if the model always say “0” or “1” it is not technically wrong, but it is not very helpful either. 7.1.2 Performance Metrics and ROC Curves Each model was evaluated using four metrics: Balanced Accuracy, F1-Score, Precision, and Recall. compute_metrics &lt;- function(obs, pred) { cm &lt;- confusionMatrix(factor(pred), factor(obs), positive = &quot;1&quot;) out &lt;- cm$byClass[c(&quot;Balanced Accuracy&quot;, &quot;F1&quot;, &quot;Precision&quot;, &quot;Recall&quot;)] return(as.list(out)) } # Test metrics test_metrics &lt;- map_dfr(names(RF_final$test), function(label) { map_dfr(RF_final$test[[label]], function(model) { m &lt;- compute_metrics(model$observed, model$predictions) tibble(Label = label, mtry = model$mtry, nodesize = model$nodesize, Phase = &quot;test&quot;, BalancedAccuracy = m$`Balanced Accuracy`, F1 = m$F1, Precision = m$Precision, Recall = m$Recall) }) }) test_metrics &lt;- test_metrics %&gt;% select(Label, mtry, nodesize, BalancedAccuracy, F1, Precision, Recall) %&gt;% pivot_longer(cols = c(BalancedAccuracy, F1, Precision, Recall), names_to = &quot;Metric&quot;, values_to = &quot;Value&quot;) plot_met &lt;- ggplot(test_metrics, aes(x = Metric, y = Value, group = interaction(mtry, nodesize), color = factor(nodesize), linetype = factor(mtry))) + geom_line(linewidth = .7, alpha = 0.7) + geom_point(size = 2, alpha = 0.7) + facet_wrap(~ Label, ncol = 1) + theme_light() + labs(y = &quot;Score&quot;, x = &#39;&#39;, color = &quot;Nodesize&quot;, linetype = &quot;Mtry&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1), strip.text = element_text(color = &#39;black&#39;), legend.position = &#39;none&#39;) + scale_color_manual(values = c(&#39;#233d4d&#39;, &#39;#fe7f2d&#39;, &#39;#83c5be&#39;)) + scale_linetype_manual(values = c(&quot;500&quot; = &quot;solid&quot;, &quot;1000&quot; = &quot;dashed&quot;, &quot;13491&quot; = &quot;dotted&quot;)) library(pROC) library(tibble) roc_all &lt;- lapply(names(RF_final$test), function(label) { models &lt;- RF_final$test[[label]] rocs &lt;- lapply(seq_along(models), function(i) { x &lt;- models[[i]] r &lt;- tryCatch({ roc_obj &lt;- roc(response = x$observed, predictor = x$predicted_prob, quiet = TRUE, levels = c(0, 1), direction = &quot;&lt;&quot;) tibble(Label = label, mtry = x$mtry, nodesize = x$nodesize, specificity = rev(roc_obj$specificities), sensitivity = rev(roc_obj$sensitivities), auc = as.numeric(auc(roc_obj)), model_id = paste0(&quot;mtry=&quot;, x$mtry, &quot;_nodesize=&quot;, x$nodesize)) }) return(r) }) bind_rows(rocs) }) roc_df &lt;- bind_rows(roc_all) roc_df &lt;- mutate(roc_df, mtry = as.factor(mtry), nodesize = as.factor(nodesize)) plot_roc &lt;- ggplot(roc_df, aes(x = 1 - specificity, y = sensitivity, color = factor(nodesize), linetype = factor(mtry))) + geom_line(alpha = 0.7, linewidth = 0.7) + facet_wrap(~ Label, ncol = 1) + theme_light() + labs(x = &quot;False Positive Rate&quot;, y = &quot;True Positive Rate&quot;, color = &quot;nodesize&quot;, linetype = &quot;mtry&quot;) + scale_color_manual(values = c(&#39;#233d4d&#39;, &#39;#fe7f2d&#39;, &#39;#83c5be&#39;)) + scale_linetype_manual(values = c(&quot;500&quot; = &quot;solid&quot;, &quot;1000&quot; = &quot;dashed&quot;, &quot;13491&quot; = &quot;dotted&quot;)) + theme(strip.text = element_text(color = &#39;black&#39;)) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine library(grid) g1 &lt;- ggplotGrob(plot_met) g2 &lt;- ggplotGrob(plot_roc) max_height &lt;- grid::unit.pmax(g1$heights, g2$heights) g1$heights &lt;- max_height g2$heights &lt;- max_height grid.arrange(g1, g2, ncol = 2) Once again, heat_1h attract attention with high and stable performance across the board. Drought conditions, on the other hand, gave the models a hard time, often pushing them into the “just predict one class and hope for the best” corner. The result? High recall, but poor precision and disappointing F1-scores. heat_6h sat somewhere in the middle: still sensitive, but less specific. These patterns suggest that early heat stress triggers a clear and strong transcriptomic response, easily picked up at the k-mer level. Drought, however, might be slower to kick in or more tangled with noise, making it harder to pin down with the current approach. 7.1.3 Important k-mers associated with heat stress (1h) To look behind the scenes and see which sequences were pulling the strings, I extracted the top 20 k-mers ranked by Mean Decrease in Gini from the best-performing Random Forest trained on heat_1h. best_model &lt;- RF_final$test[[&quot;heat_1h&quot;]][[which.max(sapply(RF_final$test[[&quot;heat_1h&quot;]], function(x) x$acc))]]$model imp &lt;- as.data.frame(best_model$importance) imp_kmers &lt;- imp %&gt;% rownames_to_column(&quot;kmer&quot;) %&gt;% filter(kmer != &quot;control&quot;) %&gt;% arrange(desc(MeanDecreaseGini)) top_kmers &lt;- imp_kmers %&gt;% slice_max(MeanDecreaseGini, n = 20) ggplot(top_kmers, aes(x = reorder(kmer, MeanDecreaseGini), y = MeanDecreaseGini)) + geom_col(fill = &quot;steelblue&quot;) + coord_flip() + theme_minimal(base_size = 12) + labs(x = &quot;k-mer (41-mer)&quot;, y = &quot;Mean Decrease in Gini&quot;) **Unfortunately, I didn’t have time to validate whether these star k-mers actually map to known genes or differentially expressed regions. And I’m genuinely frustrated about that. It would have made the story stronger, more complete and biologically grounded. That said, these k-mers are ready for follow-up work: alignment to reference transcripts, GO enrichment, or anything else that helps explain why the model liked them so much. If I had more time, this would absolutely be next. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
